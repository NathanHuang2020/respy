{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exogenous Processes\n",
    "This notebook contains prototypes for the implementation of exogenous processes.\n",
    "I think the easiest is to start implementing the processes form a new branch that departs form current main. \n",
    "The only part that we can take over one to one is the model specification and parsing. \n",
    "The rest serves as good inspiration but has to be adapted substantially due to the new state space structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import wraps\n",
    "from scipy import special\n",
    "\n",
    "import respy as rp\n",
    "\n",
    "from respy.config import COVARIATES_DOT_PRODUCT_DTYPE\n",
    "from respy.parallelization import parallelize_across_dense_dimensions\n",
    "from respy.shared import create_dense_state_space_columns\n",
    "from respy.shared import pandas_dot\n",
    "from respy.solve import _create_choice_rewards\n",
    "from respy.load_states import pandas_dot\n",
    "from respy.pre_processing.model_processing import process_params_and_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Processing\n",
    "- I would just take over the model processing from the old PR\n",
    "- We have to copy all the functions that deal with exog processes to the new branch\n",
    "- That is probably the last error prone way and it allows us to spot potential improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mo2561057/OpenSourceEconomics/respy/respy/pre_processing/model_processing.py:590: UserWarning: The probabilities for parameter group \\bobservable_ability_([0-9a-z_]+)\\b do not sum to one.\n",
      "  category=UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# Load model.\n",
    "params, options = rp.get_example_model(\"robinson_crusoe_extended\", with_data=False)\n",
    "\n",
    "# Extend with observable characteristic.\n",
    "params.loc[(\"observable_health_well\", \"probability\"), \"value\"] = 0.9\n",
    "params.loc[(\"observable_health_sick\", \"probability\"), \"value\"] = 0.1\n",
    "params.loc[(\"observable_ability_good\", \"probability\"), \"value\"] = 0.9\n",
    "params.loc[(\"observable_ability_bad\", \"probability\"), \"value\"] = 0.1\n",
    "params.loc[(\"observable_ability_horrible\", \"probability\"), \"value\"] = 0.1\n",
    "\n",
    "\n",
    "\n",
    "# Create internal specification objects.\n",
    "optim_paras, options = process_params_and_options(params, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ability', 'health'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim_paras[\"observables\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add exog processes\n",
    "sp = rp.state_space.create_state_space_class(optim_paras, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictType[UniTuple(int64 x 2),int64]<iv=None>({(0, 0): 0, (0, 1): 1, (1, 0): 2, (1, 1): 3, (2, 0): 4, (2, 1): 5})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.dense_covariates_to_dense_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "The old implementation seems to calculate transition probabilities once while retrieving continuation values. \n",
    "This approach seems impractical now since we do not keep the full representation of dense_period_choice cores in working memory.\n",
    "Furthermore I hold the opinion that it should be treated similair to wages or nonpecs to ensure readability, coherence and flexibility. \n",
    "There are essentially three different avenues we could take now:\n",
    "- We can treat the transition matrix entirely like wages and nonpecuniary rewards. The upside is that this probably\n",
    "  integrated into the code quite smoothly and that it requires the least calculations. The downside is this would be quite working\n",
    "  memory intensive.\n",
    "\n",
    "- We can treat transition probabilities like states. Upsides are reduced working memory usage and smooth integration. \n",
    "  The downside are quite some IO calls.\n",
    "  \n",
    "- We can calculate transition probabilities each period in the backwardinduction loop. Altough this does not suffer from the issues \n",
    "  above I consider this to be the messiest solution. It would also cast some issues for the simulation. \n",
    "\n",
    "\n",
    "For now I follow the first approach but I think once we agree on the general strcuture the difference should not be too large! \n",
    "Th Code cells that follow contain all additions and changes to the state space and solve modules.\n",
    "Code is not tested yet! Some small issues still have to be fixed!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imortant general features to ensure no matter what option we choose: \n",
    "-  We need to be certain that the dense grid always has fixed dense vars in the leading positions and exog processes in the last!\n",
    "   dense_covraite = (observable_position, process_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 we build a df for all potential locations on the dense grid\n",
    "def compute_transition_probabilities(states,\n",
    "                                     core_key,\n",
    "                                     complex_,\n",
    "                                     dense_covariates_to_dense_index,\n",
    "                                     dense_index_and_core_key_to_dense_key,\n",
    "                                     optim_paras,\n",
    "                                     options\n",
    "                                     ):\n",
    "    \n",
    "    exogenous_processes = optim_paras[\"exogenous_processes\"]\n",
    "    \n",
    "    # How does the accounting work here again? Would that actually work?\n",
    "    static_dense_columns = optim_paras[\"observables\"] # We also still need to add types. Rethink parsing to an extent?\n",
    "    \n",
    "    static_dense = list(states.loc[0,static_dense].values())\n",
    "    \n",
    "    dense_columns = create_dense_state_space_columns(optim_paras)\n",
    "    \n",
    "    levels_of_processes = [range(len(i)) for i in optim_paras[\"observables\"].values()]\n",
    "    comb_exog_procs = itertools.product(*levels_of_processes)\n",
    "    \n",
    "    # Needs to be created in here since that is dense-period-choice-core specific. \n",
    "    dense_index_to_exogenous = {dense_covariates_to_dense_index[(*static_dense, *exog)]:exog for exog in comb_exog_procs}\n",
    "    dense_key_to_exogenous = {dense_index_and_core_key_to_dense_key[(core_key,key)]:vaue for key,value in dense_index_to_exogenous.items()}\n",
    "    \n",
    "    # Compute the probabilities for every exogenous process.\n",
    "    probabilities = []\n",
    "    for exog_proc in exogenous_processes:\n",
    "\n",
    "        # Create the dot product of covariates and parameters.\n",
    "        x_betas = []\n",
    "        for params in exogenous_processes[exog_proc].values():\n",
    "            x_beta = pandas_dot(states[params.index],params)\n",
    "            x_betas.append(x_beta)\n",
    "\n",
    "        probs = special.softmax(np.column_stack(x_betas), axis=1)\n",
    "        probabilities.append(probs)\n",
    "    \n",
    "    # Prepare full Dataframe. If issues arrise we might want to switch typed dicts \n",
    "    df = pd.Dataframe(index=states.index)\n",
    "    for dense in dense_index_to_exogenous:\n",
    "        array = np.product.reduce(probs[proc][:,val] for proc,val in enumerate(dense_key_to_exogenous[dense]))\n",
    "        df[dense] = array\n",
    "    \n",
    "    # We can maybe  directly dump that dataset?\n",
    "    # I think we still have to discuss whether we realy want that!\n",
    "    dump_container(df,\"transition\",complex_, options)\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gefällt mir irgendwie doch nciht so gut. Wäre cool wenn wir hier noch flexibler werden könnten. \n",
    "# Auch bezüglich der meisten \n",
    "@parallelize_across_dense_dimensions\n",
    "def _create_param_specific_objects(    \n",
    "    complex_,\n",
    "    core_key,\n",
    "    choice_set,\n",
    "    dense_covariates_to_dense_index,\n",
    "    dense_index_and_core_key_to_dense_key,\n",
    "    optim_paras,\n",
    "    options):\n",
    "    \"\"\" Insert Docstring \"\"\"\n",
    "    states = load_states(complex_, options)\n",
    "    wages, nonpecs = _create_choice_rewards(complex_, choice_set, optim_paras, options)\n",
    "    if \"exogenous_processes\" in optim_paras:\n",
    "        transition = compute_transition_probabilities(states,\n",
    "                                        core_key,\n",
    "                                         complex_,\n",
    "                                         dense_covariates_to_dense_index,\n",
    "                                         dense_index_and_core_key_to_dense_key,\n",
    "                                         optim_paras,\n",
    "                                         options)\n",
    "    return wages, nonpecs, transition\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(params, options, state_space):\n",
    "    \"\"\"Solve the model.\"\"\"\n",
    "    optim_paras, options = process_params_and_options(params, options)\n",
    "        \n",
    "    # TODO: Think how this could be used to handle fixed and floating variables more efficiently. \n",
    "    # Thinking about Polymorphisms etc. \n",
    "    wages, nonpecs, transition = _create_param_specific_objects(\n",
    "        state_space.dense_key_to_complex,\n",
    "        state_space.dense_key_to_core_key,\n",
    "        state_space.dense_key_to_choice_set,\n",
    "        state_space.dense_covariates_to_dense_index,\n",
    "        state_space.dense_index_and_core_key_to_dense_key,\n",
    "        optim_paras,\n",
    "        options,\n",
    "    )\n",
    "\n",
    "    state_space.wages = wages\n",
    "    state_space.nonpecs = nonpecs\n",
    "    state_space.transition = transition\n",
    "\n",
    "    state_space = _solve_with_backward_induction(state_space, optim_paras, options)\n",
    "\n",
    "    return state_space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need a way to efficiently combine values!\n",
    "I would propose to use another decorator to do the weighting. I think that avoids a large mess in the solve module and \n",
    "it does justice to the abstract extension of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Think of a less specific name. Something like \n",
    "def weight_dense_cores(func):\n",
    "    \"\"\"Wrapper around get continuation values\"\"\"\n",
    "    @wraps(func)\n",
    "    def decorator_weight_dense_cores(*args, transition, **kwargs):\n",
    "        exogenous = True if transition is not None\n",
    "        continuation_values = func(*args, **kwargs) \n",
    "        if is exogenous:\n",
    "            weighted_continuation_values = continuation_values.copy() # Not sure whether this is a nice solution \n",
    "            for dense_key, transition_df in transition.items():\n",
    "                weighted_columns = \\\n",
    "                [transition_df[ftr_key].values.reshape(len(transition_df),1)*continuation_values[ftr_key] for ftr_key in transition_df.columns]\n",
    "                weighted_continuation_values[dense_key] = np.sum.reduce(weighted_columns)\n",
    "            return weighted_continuation_values\n",
    "        else:\n",
    "            return continuation values\n",
    "    return wrap_continuation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_continuation_values(self, period, transition):\n",
    "        \"\"\"Get continuation values.\n",
    "\n",
    "        The function takes the expected value functions from the previous periods and\n",
    "        then uses the indices of child states to put these expected value functions in\n",
    "        the correct format. If period is equal to self.n_periods - 1 the function\n",
    "        returns arrays of zeros si        self.expected_value_functions = Dict.empty(\n",
    "            key_type=nb.types.int64, value_type=nb.types.float64[:]\n",
    "        )\n",
    "        for index, indices in self.dense_key_to_core_indices.items():\n",
    "            self.expected_value_functions[index] = np.zeros(len(indices))nce we are in terminal states. Otherwise we retrieve\n",
    "        expected value functions for next period and call\n",
    "        :func:`_get_continuation_values` to assign continuation values to all choices\n",
    "        within a period. (The object `subset_expected_value_functions` is required\n",
    "        because we need a Numba typed dict but the function\n",
    "        :meth:`StateSpace.get_attribute_from_period` just returns a normal dict)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        continuation_values : numba.typed.Dict\n",
    "            The continuation values for each dense key in a :class:`numpy.ndarray`.\n",
    "\n",
    "        See also\n",
    "        --------\n",
    "        _get_continuation_values\n",
    "            A more theoretical explanation can be found here: See :ref:`get continuation\n",
    "            values <get_continuation_values>`.\n",
    "\n",
    "        \"\"\"\n",
    "        if period == self.n_periods - 1:\n",
    "            shapes = self.get_attribute_from_period(\"base_draws_sol\", period)\n",
    "            states = self.get_attribute_from_period(\"dense_key_to_core_indices\", period)\n",
    "            continuation_values = {\n",
    "                key: np.zeros((states[key].shape[0], shapes[key].shape[1]))\n",
    "                for key in shapes\n",
    "            }\n",
    "        else:\n",
    "            child_indices = self.get_attribute_from_period(\"child_indices\", period)\n",
    "            expected_value_functions = self.get_attribute_from_period(\n",
    "                \"expected_value_functions\", period + 1\n",
    "            )\n",
    "            subset_expected_value_functions = Dict.empty(\n",
    "                key_type=nb.types.int64, value_type=nb.types.float64[:]\n",
    "            )\n",
    "            for key, value in expected_value_functions.items():\n",
    "                subset_expected_value_functions[key] = value\n",
    "\n",
    "            continuation_values = _get_continuation_values(\n",
    "                self.get_attribute_from_period(\"dense_key_to_core_indices\", period),\n",
    "                self.get_attribute_from_period(\"dense_key_to_complex\", period),\n",
    "                child_indices,\n",
    "                self.core_key_and_dense_index_to_dense_key,\n",
    "                transition,\n",
    "                bypass={\"expected_value_functions\": subset_expected_value_functions},\n",
    "            )\n",
    "        return continuation_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@weight_dense_cores\n",
    "@parallelize_across_dense_dimensions\n",
    "@nb.njit\n",
    "def _get_continuation_values(\n",
    "    core_indices,\n",
    "    dense_complex_index,\n",
    "    child_indices,\n",
    "    core_index_and_dense_vector_to_dense_index,\n",
    "    expected_value_functions,\n",
    "):\n",
    "    \"\"\"Get continuation values from child states.\n",
    "\n",
    "    The continuation values are the discounted expected value functions from child\n",
    "    states. This method allows to retrieve continuation values that were obtained in the\n",
    "    model solution. In particular the function assigns continuation values to state\n",
    "    choice combinations by using the child indices created in\n",
    "    :func:`_collect_child_indices`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    continuation_values : numpy.ndarray\n",
    "        Array with shape ``(n_states, n_choices)``. Maps core_key and choice into\n",
    "        continuation value.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(dense_complex_index) == 3:\n",
    "        period, choice_set, dense_idx = dense_complex_index\n",
    "    elif len(dense_complex_index) == 2:\n",
    "        period, choice_set = dense_complex_index\n",
    "        dense_idx = 0\n",
    "\n",
    "    n_choices = sum_over_numba_boolean_unituple(choice_set)\n",
    "\n",
    "    n_states = core_indices.shape[0]\n",
    "\n",
    "    continuation_values = np.zeros((len(core_indices), n_choices))\n",
    "    for i in range(n_states):\n",
    "        for j in range(n_choices):\n",
    "            core_idx, row_idx = child_indices[i, j]\n",
    "            idx = (core_idx, dense_idx)\n",
    "            dense_choice = core_index_and_dense_vector_to_dense_index[idx]\n",
    "\n",
    "            continuation_values[i, j] = expected_value_functions[dense_choice][row_idx]\n",
    "\n",
    "    return continuation_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "The simulation constitutes a somewhat different issue in terms of splitting the model. \n",
    "I would propose to calculate transition probabilities in the _simulate_single_period step! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@split_and_combine_df\n",
    "@parallelize_across_dense_dimensions\n",
    "def _simulate_single_period(\n",
    "    df, choice_set, wages, nonpecs, continuation_values, optim_paras\n",
    "):\n",
    "    \"\"\"Simulate individuals in a single period.\n",
    "\n",
    "    The function performs the following sets:\n",
    "\n",
    "    - Map individuals in one period to the states in the model.\n",
    "    - Simulate choices and wages for those individuals.\n",
    "    - Store additional information in a :class:`pandas.DataFrame` and return it.\n",
    "\n",
    "    Until now this function assumes that there are no mixed constraints.\n",
    "    See docs for more information!\n",
    "\n",
    "    \"\"\"\n",
    "    valid_choices = select_valid_choices(optim_paras[\"choices\"], choice_set)\n",
    "\n",
    "    n_wages_raw = len(optim_paras[\"choices_w_wage\"])\n",
    "    n_wages = sum(choice_set[:n_wages_raw])\n",
    "\n",
    "    # Get indices which connect states in the state space and simulated agents. Subtract\n",
    "    # the minimum of indices (excluding invalid indices) because wages, etc. contain\n",
    "    # only wages in this period and normal indices select rows from all wages.\n",
    "\n",
    "    period_indices = df[\"core_index\"].to_numpy()\n",
    "    try:\n",
    "        wages = wages[period_indices]\n",
    "        nonpecs = nonpecs[period_indices]\n",
    "        continuation_values = continuation_values[period_indices]\n",
    "    except IndexError as e:\n",
    "        raise Exception(\n",
    "            \"Simulated individuals could not be mapped to their corresponding states in\"\n",
    "            \" the state space. This might be caused by a mismatch between \"\n",
    "            \"option['core_state_space_filters'] and the initial conditions.\"\n",
    "        ) from e\n",
    "\n",
    "    draws_shock = df[[f\"shock_reward_{c}\" for c in valid_choices]].to_numpy()\n",
    "    draws_shock_transformed = transform_base_draws_with_cholesky_factor(\n",
    "        draws_shock, choice_set, optim_paras[\"shocks_cholesky\"], optim_paras\n",
    "    )\n",
    "\n",
    "    draws_wage = df[[f\"meas_error_wage_{c}\" for c in valid_choices]].to_numpy()\n",
    "    value_functions, flow_utilities = calculate_value_functions_and_flow_utilities(\n",
    "        wages,\n",
    "        nonpecs,\n",
    "        continuation_values,\n",
    "        draws_shock_transformed,\n",
    "        optim_paras[\"beta_delta\"],\n",
    "    )\n",
    "    choice = np.nanargmax(value_functions, axis=1)\n",
    "\n",
    "    # Get choice replacement dict. There is too much positioning until now!\n",
    "    wages = wages * draws_shock_transformed * draws_wage\n",
    "    wages[:, n_wages:] = np.nan\n",
    "    wage = np.choose(choice, wages.T)\n",
    "\n",
    "    # We map choice positions to choice codes\n",
    "    positions = [i for i, x in enumerate(optim_paras[\"choices\"]) if x in valid_choices]\n",
    "    for pos, val in enumerate(positions):\n",
    "        choice = np.where(choice == pos, val, choice)\n",
    "\n",
    "    # Store necessary information and information for debugging, etc..\n",
    "\n",
    "    df[\"choice\"] = choice\n",
    "    df[\"wage\"] = wage\n",
    "    df[\"discount_rate\"] = optim_paras[\"delta\"]\n",
    "    df[\"present_bias\"] = optim_paras[\"beta\"]\n",
    "\n",
    "    for i, choice in enumerate(valid_choices):\n",
    "        df[f\"nonpecuniary_reward_{choice}\"] = nonpecs[:, i]\n",
    "        df[f\"wage_{choice}\"] = wages[:, i]\n",
    "        df[f\"flow_utility_{choice}\"] = flow_utilities[:, i]\n",
    "        df[f\"value_function_{choice}\"] = value_functions[:, i]\n",
    "        df[f\"continuation_value_{choice}\"] = continuation_values[:, i]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
