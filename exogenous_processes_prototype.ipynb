{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exogenous Processes\n",
    "This notebook contains prototypes for the implementation of exogenous processes.\n",
    "I think the easiest is to start implementing the processes form a new branch that departs form current main. \n",
    "The only part that we can take over one to one is the model specification and parsing. \n",
    "The rest serves as good inspiration but has to be adapted substantially due to the new state space structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import wraps\n",
    "from scipy import special\n",
    "\n",
    "import respy as rp\n",
    "\n",
    "from respy.config import COVARIATES_DOT_PRODUCT_DTYPE\n",
    "from respy.parallelization import parallelize_across_dense_dimensions\n",
    "from respy.shared import create_dense_state_space_columns\n",
    "from respy.pre_processing.model_processing import process_params_and_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Processing\n",
    "- I would just take over the model processing from the old PR\n",
    "- We have to copy all the functions that deal with exog processes to the new branch\n",
    "- That is probably the last error prone way and it allows us to spot potential improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mo2561057/OpenSourceEconomics/respy/respy/pre_processing/model_processing.py:590: UserWarning: The probabilities for parameter group \\bobservable_ability_([0-9a-z_]+)\\b do not sum to one.\n",
      "  category=UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# Load model.\n",
    "params, options = rp.get_example_model(\"robinson_crusoe_extended\", with_data=False)\n",
    "\n",
    "# Extend with observable characteristic.\n",
    "params.loc[(\"observable_health_well\", \"probability\"), \"value\"] = 0.9\n",
    "params.loc[(\"observable_health_sick\", \"probability\"), \"value\"] = 0.1\n",
    "params.loc[(\"observable_ability_good\", \"probability\"), \"value\"] = 0.9\n",
    "params.loc[(\"observable_ability_bad\", \"probability\"), \"value\"] = 0.1\n",
    "params.loc[(\"observable_ability_horrible\", \"probability\"), \"value\"] = 0.1\n",
    "\n",
    "\n",
    "\n",
    "# Create internal specification objects.\n",
    "optim_paras, options = process_params_and_options(params, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ability', 'health'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim_paras[\"observables\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add exog processes\n",
    "sp = rp.state_space.create_state_space_class(optim_paras, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictType[UniTuple(int64 x 2),int64]<iv=None>({(0, 0): 0, (0, 1): 1, (1, 0): 2, (1, 1): 3, (2, 0): 4, (2, 1): 5})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.dense_covariates_to_dense_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "I think we face a memory speed trade off here after all.\n",
    "Either we directly create the dataframe for all potential densen indices or we keep different processes seperated. \n",
    "The former could be faster while the latter uses much less memory. (Just a hunch tho) We should try both anyways!\n",
    "\n",
    "Imortant features to ensure: \n",
    "- We need to be certain that the dense grid always has fixed dense vars in the leading positions and exog processes in the last!\n",
    "dense_covraite = (observable_position, process_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need an efficient way to map dense covariates to dense indices!\n",
    "# We need a dict indicating position in dense vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 we build a df for all potential locations on the dense grid\n",
    "@parallelize_across_dense_dimensions\n",
    "def compute_process_specific_transition_probabilities(states,\n",
    "                                                      core_key,\n",
    "                                                      dense_covariates_to_dense_index,\n",
    "                                                      dense_index_and_core_key_to_dense_key,\n",
    "                                                      optim_paras\n",
    "                                                     ):\n",
    "    exogenous_processes = optim_paras[\"exogenous_processes\"]\n",
    "    \n",
    "    # How does the accounting work here again? Would that actually work?\n",
    "    static_dense_columns = optim_paras[\"observables\"] # We also still need to add types. Rethink parsing to an extent?\n",
    "    \n",
    "    static_dense = list(states.loc[0,static_dense].values())\n",
    "    \n",
    "    dense_columns = create_dense_state_space_columns(optim_paras)\n",
    "    \n",
    "    levels_of_processes = [range(len(i)) for i in optim_paras[\"observables\"].values()]\n",
    "    comb_exog_procs = itertools.product(*levels_of_processes)\n",
    "    \n",
    "    # Needs to be created in here since that is dense-period-choice-core specific. \n",
    "    dense_index_to_exogenous = {dense_covariates_to_dense_index[(*static_dense, *exog)]:exog for exog in comb_exog_procs}\n",
    "    dense_key_to_exogenous = {dense_index_and_core_key_to_dense_key[(core_key,key)]:vaue for key,value in dense_index_to_exogenous.items()}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute the probabilities for every exogenous process.\n",
    "    probabilities = []\n",
    "    for exog_proc in exogenous_processes:\n",
    "\n",
    "        # Create the dot product of covariates and parameters.\n",
    "        x_betas = []\n",
    "        for params in exogenous_processes[exog_proc].values():\n",
    "            x_beta = np.dot(\n",
    "                states[params.index].to_numpy(dtype=COVARIATES_DOT_PRODUCT_DTYPE),\n",
    "                params.to_numpy(),\n",
    "            )\n",
    "            x_betas.append(x_beta)\n",
    "\n",
    "        probs = special.softmax(np.column_stack(x_betas), axis=1)\n",
    "        probabilities.append(probs)\n",
    "    \n",
    "    # Prepare full Dataframe\n",
    "    df = pd.Dataframe(index=states.index)\n",
    "    for dense in dense_index_to_exogenous:\n",
    "        array = np.product.reduce(probs[proc][:,val] for proc,val in enumerate(dense_key_to_exogenous[dense]))\n",
    "        df[dense] = array\n",
    "    \n",
    "    # We can maybe  directly dump that dataset? \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position of creation\n",
    "One question is whether we want to create the transition at runtime or whether we want to create it before and store it on disk. I slightly prefer the latter. Altough it is slightly slower I think it allows for more flexibility down the road. \n",
    "Especially when we want to allow for fixed processes it is crucial to create transitions before.\n",
    "That is why I propose the following changes to the dumping of functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_file_name_from_complex_index(topic, complex_):\n",
    "    \"\"\"Create a file name from a complex index.\"\"\"\n",
    "    choice = \"\".join([str(int(x)) for x in complex_[1]])\n",
    "    if len(complex_) == 3:\n",
    "        file_name = f\"{topic}_{complex_[0]}_{choice}_{complex_[2]}.parquet\"\n",
    "    elif len(complex_) == 2:\n",
    "        file_name = f\"{topic}_{complex_[0]}_{choice}.parquet\"\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_states(container, topic, complex_, options):\n",
    "    \"\"\"Dump states.\"\"\"\n",
    "    file_name = _create_file_name_from_complex_index(complex_)\n",
    "    states.to_parquet(\n",
    "        options[\"cache_path\"] / file_name, compression=options[\"cache_compression\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def load_states(topic, complex_, options):\n",
    "    \"\"\"Load states.\"\"\"\n",
    "    file_name = _create_file_name_from_complex_index(topic, complex_)\n",
    "    directory = options[\"cache_path\"]\n",
    "    return pd.read_parquet(directory / file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need a way to efficiently combine values!\n",
    "I would propose to use another decorator to do the weighting. I think that avoids a large mess in the solve module and \n",
    "it does justice to the abstract extension of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Think of a less specific name. Something like \n",
    "def weight_dense_cores(func):\n",
    "    \"\"\"Wrapper around get continuation values\"\"\"\n",
    "    @wraps(func)\n",
    "    def decorator_weight_dense_cores(state_space, dense_key_to_complex, *args):\n",
    "        is_exogenous = \"exogenous_processes\" in state_space.optim_paras.keys()\n",
    "        continuation_values = func(state_space, complex, *args) \n",
    "        if is exogenous:\n",
    "            weighted_continuation_values = dict() # Will probably have to become a numba typed dict\n",
    "            for dense_key in dense_key_to_complex:\n",
    "                complex_ = dense_key_to_complex[dense_key]\n",
    "                transition_df = load_states(\"transition\",complex_)\n",
    "                #TODO: Find a more elegant way than list comprehension\n",
    "                weighted_columns = [np.dot(transition_df[ftr_key].to_numpy(),continuation_values[ftr_key]) for ftr_key in transition_df.columns]\n",
    "                weighted_continuation_values[dense_key] = np.sum.reduce(weighted_columns)\n",
    "            return weighted_continuation_values\n",
    "        else:\n",
    "            return continuation values\n",
    "    return wrap_continuation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@weight_continuation_values\n",
    "@parallelize_across_dense_dimensions\n",
    "@nb.njit\n",
    "def _get_continuation_values(\n",
    "    core_indices,\n",
    "    dense_complex_index,\n",
    "    child_indices,\n",
    "    core_index_and_dense_vector_to_dense_index,\n",
    "    expected_value_functions,\n",
    "):\n",
    "    \"\"\"Get continuation values from child states.\n",
    "\n",
    "    The continuation values are the discounted expected value functions from child\n",
    "    states. This method allows to retrieve continuation values that were obtained in the\n",
    "    model solution. In particular the function assigns continuation values to state\n",
    "    choice combinations by using the child indices created in\n",
    "    :func:`_collect_child_indices`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    continuation_values : numpy.ndarray\n",
    "        Array with shape ``(n_states, n_choices)``. Maps core_key and choice into\n",
    "        continuation value.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(dense_complex_index) == 3:\n",
    "        period, choice_set, dense_idx = dense_complex_index\n",
    "    elif len(dense_complex_index) == 2:\n",
    "        period, choice_set = dense_complex_index\n",
    "        dense_idx = 0\n",
    "\n",
    "    n_choices = sum_over_numba_boolean_unituple(choice_set)\n",
    "\n",
    "    n_states = core_indices.shape[0]\n",
    "\n",
    "    continuation_values = np.zeros((len(core_indices), n_choices))\n",
    "    for i in range(n_states):\n",
    "        for j in range(n_choices):\n",
    "            core_idx, row_idx = child_indices[i, j]\n",
    "            idx = (core_idx, dense_idx)\n",
    "            dense_choice = core_index_and_dense_vector_to_dense_index[idx]\n",
    "\n",
    "            continuation_values[i, j] = expected_value_functions[dense_choice][row_idx]\n",
    "\n",
    "    return continuation_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zzzzz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
